{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7580d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# # Open original and new CSV files\n",
    "# with open('slogon.csv', 'r', encoding='utf-8') as infile, open('new_slogon.csv', 'w', newline='', encoding='utf-8') as outfile:\n",
    "#     reader = csv.reader(infile)\n",
    "#     writer = csv.writer(outfile)\n",
    "    \n",
    "#     # Write new header\n",
    "#     writer.writerow(['prompt', 'slogon'])\n",
    "\n",
    "#     next(reader)  # Skip the header line\n",
    "\n",
    "#     for row in reader:\n",
    "#         try:\n",
    "#             # Example row: ['logo_00000', 'a logo...', 'slogan...']\n",
    "#             # If your row only has 1 item (i.e., full line), split it manually\n",
    "#             if len(row) == 1:\n",
    "#                 parts = row[0].split(',\"')\n",
    "#                 if len(parts) == 3:\n",
    "#                     prompt = parts[1].strip('\"')\n",
    "#                     slogon = parts[2].strip('\"')\n",
    "#                     writer.writerow([prompt, slogon])\n",
    "#             elif len(row) >= 3:\n",
    "#                 # Already split correctly\n",
    "#                 writer.writerow([row[1], row[2]])\n",
    "#         except Exception as e:\n",
    "#             print(f\"Skipping malformed row: {row} — Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0614e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prompt', 'slogon']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV with correct delimiter\n",
    "df = pd.read_csv('new_slogon.csv', delimiter=',')\n",
    "\n",
    "# Get and print column names\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b8d4742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>slogon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a logo of coffee shop, take-away coffee cardbo...</td>\n",
       "      <td>Fuel Every Moment, One Sip at a Time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a logo of coffee shop, White round background ...</td>\n",
       "      <td>Brew &amp; Bite: Where Warmth Meets Delight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a logo of coffee shop, image of a filled cup w...</td>\n",
       "      <td>Rise and Shine, Sip by Sip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a logo of cafe restaurant bar pizzeria with a ...</td>\n",
       "      <td>Where Every Slice Marks a New Chapter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a logo of cafe restaurant bar with a circle wi...</td>\n",
       "      <td>Gather to Savor, Return for the Flavor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  a logo of coffee shop, take-away coffee cardbo...   \n",
       "1  a logo of coffee shop, White round background ...   \n",
       "2  a logo of coffee shop, image of a filled cup w...   \n",
       "3  a logo of cafe restaurant bar pizzeria with a ...   \n",
       "4  a logo of cafe restaurant bar with a circle wi...   \n",
       "\n",
       "                                    slogon  \n",
       "0     Fuel Every Moment, One Sip at a Time  \n",
       "1  Brew & Bite: Where Warmth Meets Delight  \n",
       "2               Rise and Shine, Sip by Sip  \n",
       "3    Where Every Slice Marks a New Chapter  \n",
       "4   Gather to Savor, Return for the Flavor  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5a0f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"new_slogon.csv\")\n",
    "\n",
    "# Clean up any rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Rename columns to match expected format\n",
    "df = df.rename(columns={\"prompt\": \"input_text\", \"slogon\": \"target_text\"})\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Split into train and validation\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"test\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe74cd11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693c6435e11d4c999ad649abd277aa0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/722 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734685be91b24bf193f022e8b11b61ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/81 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "model_checkpoint = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Tokenize the inputs and targets\n",
    "max_input_length = 64\n",
    "max_target_length = 32\n",
    "\n",
    "def preprocess(batch):\n",
    "    inputs = tokenizer(batch[\"input_text\"], max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
    "    targets = tokenizer(batch[\"target_text\"], max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess, batched=True)\n",
    "tokenized_val = val_dataset.map(preprocess, batched=True)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14ed0bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nisha\\AppData\\Local\\Temp\\ipykernel_4396\\3316486813.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1810' max='1810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1810/1810 41:40, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>16.454000</td>\n",
       "      <td>1.465001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.566500</td>\n",
       "      <td>0.745006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.667785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.722700</td>\n",
       "      <td>0.645550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.680100</td>\n",
       "      <td>0.633906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.645200</td>\n",
       "      <td>0.628949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.636200</td>\n",
       "      <td>0.626490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.632100</td>\n",
       "      <td>0.622929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.609400</td>\n",
       "      <td>0.622389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.606400</td>\n",
       "      <td>0.622057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1810, training_loss=1.7612010176010553, metrics={'train_runtime': 2501.7657, 'train_samples_per_second': 2.886, 'train_steps_per_second': 0.723, 'total_flos': 617993510584320.0, 'train_loss': 1.7612010176010553, 'epoch': 10.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./slogan-generator\",\n",
    "    eval_strategy=\"epoch\",              # Evaluate after each epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,            # Reduce if you face memory issues\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=10,                      # 🔼 Increase from 4 → 10\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",                 # Better logging visibility\n",
    "    logging_steps=100,                        # Log every 100 steps\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\",                         # Disable external logging\n",
    "    generation_max_length=32,                 # Ensure generation is controlled\n",
    "    generation_num_beams=5                    # Use beam search during evaluation\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be1c7fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./slogan-generator/checkpoint-final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0267948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Slogan: Elegance in Every Face\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./slogan-generator/checkpoint-final\")\n",
    "\n",
    "# Generate slogan\n",
    "prompt = \"logo of beauty spa with a face silhouette and lotus flower inside a soft pink elli\"\n",
    "input_text = \"Generate a slogan for: \" + prompt\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(**inputs, max_length=32, num_beams=5)\n",
    "slogan = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"📝 Slogan:\", slogan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ca38882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Slogan 1: Laugh on the Best\n",
      "📝 Slogan 2: Elegance and Beauty\n",
      "📝 Slogan 3: Smile with Love\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./slogan-generator/checkpoint-final\")\n",
    "\n",
    "# Prompt\n",
    "prompt = \"logo of beauty spa with a face silhouette and lotus flower inside a soft pink elli\"\n",
    "input_text = \"Generate a slogan for: \" + prompt\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Generate 3 different slogans\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=32,\n",
    "    num_return_sequences=3,\n",
    "    do_sample=True,         # Enable sampling for diversity\n",
    "    top_k=50,               # Limit to top 50 tokens\n",
    "    temperature=0.9         # Add randomness (higher = more random)\n",
    ")\n",
    "\n",
    "# Decode and print slogans\n",
    "for i, output in enumerate(outputs, 1):\n",
    "    slogan = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    print(f\"📝 Slogan {i}:\", slogan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34422187",
   "metadata": {},
   "outputs": [],
   "source": [
    "dawdasjk ck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81976b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "GPU name: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA not available. Running on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd7d6874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Current Device: 0\n",
      "Device Name: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Current Device:\", torch.cuda.current_device())\n",
    "print(\"Device Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0c665e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request, jsonify\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from diffusers import StableDiffusionPipeline, EulerAncestralDiscreteScheduler\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Initialize models (could be moved to a separate service for better performance)\n",
    "def init_models():\n",
    "    # Slogan generator\n",
    "    slogan_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "    slogan_model = AutoModelForSeq2SeqLM.from_pretrained(\"./slogan_model/checkpoint-final\")\n",
    "    \n",
    "    # Logo generator\n",
    "    logo_pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        \"stabilityai/stable-diffusion-2-1-base\",\n",
    "        torch_dtype=torch.float16\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    # Load LoRA weights if available\n",
    "    adapter_path = \"./logo_model/checkpoint-94000\"\n",
    "    weight_file = \"pytorch_lora_weights.safetensors\"\n",
    "    if os.path.exists(os.path.join(adapter_path, weight_file)):\n",
    "        logo_pipe.load_lora_weights(adapter_path, weight_name=weight_file)\n",
    "    \n",
    "    logo_pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(logo_pipe.scheduler.config)\n",
    "    \n",
    "    return slogan_tokenizer, slogan_model, logo_pipe\n",
    "\n",
    "slogan_tokenizer, slogan_model, logo_pipe = init_models()\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/generate', methods=['POST'])\n",
    "def generate():\n",
    "    data = request.json\n",
    "    prompt = data['prompt']\n",
    "    \n",
    "    # Generate slogans\n",
    "    input_text = \"Generate a slogan for: \" + prompt\n",
    "    inputs = slogan_tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    slogan_outputs = slogan_model.generate(\n",
    "        **inputs,\n",
    "        max_length=32,\n",
    "        num_return_sequences=3,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        temperature=0.9\n",
    "    )\n",
    "    \n",
    "    slogans = [slogan_tokenizer.decode(output, skip_special_tokens=True) \n",
    "               for output in slogan_outputs]\n",
    "    \n",
    "    # Generate logos\n",
    "    negative_prompt = \"low quality, worst quality, bad composition, extra digit, fewer digits, text, inscription, watermark, label, asymmetric\"\n",
    "    \n",
    "    logo_images = []\n",
    "    for _ in range(3):\n",
    "        result = logo_pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            num_inference_steps=30,\n",
    "            guidance_scale=7.5,\n",
    "            height=512,\n",
    "            width=512\n",
    "        )\n",
    "        img = result.images[0]\n",
    "        \n",
    "        # Convert to base64 for web display\n",
    "        buffered = io.BytesIO()\n",
    "        img.save(buffered, format=\"PNG\")\n",
    "        img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "        logo_images.append(img_str)\n",
    "    \n",
    "    return jsonify({\n",
    "        'slogans': slogans,\n",
    "        'logos': logo_images\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
